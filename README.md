# Ejercicio-k-Nearest-Neighbor
## Clasificar con K-Nearest-Neighbor ejemplo en Python


ejercicio tomado del blog aprendemachinelearning
---
K-Nearest-Neighbor es un algoritmo basado en instancia de tipo supervisado de Machine Learning. Puede usarse para clasificar nuevas muestras (valores discretos) o para predecir 
(regresiÃ³n, valores continuos). Al ser un mÃ©todo sencillo, es ideal para introducirse en el mundo del  Aprendizaje AutomÃ¡tico. Sirve esencialmente para clasificar valores buscando los puntos de datos
â€œmÃ¡s similaresâ€ (por cercanÃ­a) aprendidos en la etapa de entrenamiento (ver 7 pasos para crear tu ML) y haciendo conjeturas de nuevos puntos basado en esa clasificaciÃ³n.

---
## Â¿QuÃ© es el algoritmo k-Nearest Neighbor ?

Es un mÃ©todo que simplemente busca en las observaciones mÃ¡s cercanas a la que se estÃ¡ tratando de predecir y clasifica el punto de interÃ©s basado en la mayorÃ­a de datos que le rodean. Como dijimos antes, es un algoritmo:

    Supervisado: esto -brevemente- quiere decir que tenemos etiquetado nuestro conjunto de datos de entrenamiento, con la clase o resultado esperado dada â€œuna filaâ€ de datos.
    Basado en Instancia: Esto quiere decir que nuestro algoritmo no aprende explÃ­citamente un modelo (como por ejemplo en RegresiÃ³n LogÃ­stica o Ã¡rboles de decisiÃ³n). 
    En cambio memoriza las instancias de entrenamiento que son usadas como â€œbase de conocimientoâ€ para la fase de predicciÃ³n.

---

## Â¿DÃ³nde se aplica k-Nearest Neighbor?

Aunque sencillo, se utiliza en la resoluciÃ³n de multitud de problemas, como en sistemas de recomendaciÃ³n, bÃºsqueda semÃ¡ntica y detecciÃ³n de anomalÃ­as.
Pros y contras

Como pros tiene sobre todo que es sencillo de aprender e implementar. Tiene como contras que utiliza todo el dataset para entrenar â€œcada puntoâ€ y por eso requiere de uso de mucha memoria y recursos de procesamiento (CPU). 
Por estas razones kNN tiende a funcionar mejor en datasets pequeÃ±os y sin una cantidad enorme de features (las columnas).
---
## Â¿CÃ³mo funciona kNN?

    Calcular la distancia entre el item a clasificar y el resto de items del dataset de entrenamiento.
    Seleccionar los â€œkâ€ elementos mÃ¡s cercanos (con menor distancia, segÃºn la funciÃ³n que se use)
    Realizar una â€œvotaciÃ³n de mayorÃ­aâ€ entre los k puntos: los de una clase/etiqueta que <<dominen>> decidirÃ¡n su clasificaciÃ³n final.

Teniendo en cuenta el punto 3, veremos que para decidir la clase de un punto es muy importante el valor de k, pues este terminarÃ¡ casi por definir a quÃ© grupo pertenecerÃ¡n los puntos,
sobre todo en las â€œfronterasâ€ entre grupos. Por ejemplo -y a priori- yo elegirÃ­a valores impares de k para desempatar (si las features que utilizamos son pares). No serÃ¡ lo mismo tomar para decidir 3 valores que 13. 
Esto no quiere decir que necesariamente tomar mÃ¡s puntos implique mejorar la precisiÃ³n. Lo que es seguro es que cuantos mÃ¡s â€œpuntos kâ€, mÃ¡s tardarÃ¡ nuestro algoritmo en procesar y darnos respuesta ğŸ˜‰

Las formas mÃ¡s populares de â€œmedir la cercanÃ­aâ€ entre puntos son la distancia Euclidiana (la â€œde siempreâ€) o la Cosine Similarity (mide el Ã¡ngulo de  los vectores, cuanto menores, serÃ¡n similares). 
Recordemos que este algoritmo -y prÃ¡cticamente todos en ML- funcionan mejor con varias caracterÃ­sticas de las que tomemos datos (las columnas de nuestro dataset). Lo que entendemos como â€œdistanciaâ€ en la vida real,
quedarÃ¡ abstracto a muchas dimensiones que no podemos â€œvisualizarâ€ fÃ¡cilmente (como por ejemplo en un mapa).
